---
layout: post
title: python 웹 크롤링 첫경험
date: 2020-07-19 11:13:00
author: "SeWonKim"
categories: [Devlog, ETC]
tags: [web, python]
comments: true
description: SSAFY start camp - 웹 크롤링 입문
---

바야흐로 4차 산업혁명의 시대...      
Industry 4.0...

웹에서 정보를 긁어오는 크롤링(스크래핑) 기술을 배우면서 4차 산업혁명의 인재상에 한발짝 가까워지게 되었다.


## 사전 준비
1. google drive에 Colab 추가하기
![colab](https://user-images.githubusercontent.com/30452963/87865607-87ec6680-c9b2-11ea-87ef-8f702bc56100.png)

## Colab에서 python 파일 생성
![image](https://user-images.githubusercontent.com/30452963/87865776-0269b600-c9b4-11ea-8257-6d2bc378e0a2.png)

컴퓨터에 python을 설치하지 않아도 바로 테스트 할 수 있다.

## Python package 설치
- BeautifulSoup
- requests

![image](https://user-images.githubusercontent.com/30452963/87866326-56c46400-c9bb-11ea-9c0f-65ed27ed3d3b.png)

colab에서 바로 pip install 코드를 작성하고 Ctrl+Enter를 누르면 코드를 실행할 수 있다.

## 코드 작성
```python
from bs4 import BeautifulSoup
import requests

url = "http://google.com"

# 서버의 response 정보를 이용해 response 객체 생성
response = requests.get(url)

# html 코드의 해당 부분 추출
data = response.text

# bs4를 이용해 html 코드를 soup 객체로 파싱
soup = BeautifulSoup(data, 'html.parser')

# 문서 내의 모든 <a> 태그 정보를 찾아 list로 반환
tags = soup.find_all('a')

# <a> 태그의 href 속성 값에서 url 정보 추출
for tag in tags:
  print(tag.get('href'))
```

requests로 url의 코드를 긁어오면     
BeautifulSoup가 tree 형태로 파싱해준다.     
파싱한 것을 기반으로 찾고자 하는 정보를 추출해주면 크롤링 끝!

---

라이브러리나 python 문법에 대해서 자세히 배운 건 아니라서 좀 더 공부가 필요하다.     
옛날부터 배우고 싶었던 거라서 재미있게 배웠다😁      
Thank you SSAFY ㅋㅋㅋ!
